{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing.concept_generation import process_lichess_puzzles, perform_concept_detection_pytorch_probe, extract_and_save_activations_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifically for running training, requires strong gpu, inadvisable to run\n",
    "\n",
    "import os\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "LICHESS_DB_PATH = 'lichess_db_puzzle.csv'\n",
    "ONNX_MODEL_PATH = 'prepared_network_fp16.onnx'\n",
    "CONCEPT_OUTPUT_DIR = \"concepts_sequential\"\n",
    "os.makedirs(CONCEPT_OUTPUT_DIR, exist_ok=True)\n",
    "BASE_DATA_DIR = os.path.join(CONCEPT_OUTPUT_DIR, 'processed_data')\n",
    "ACTIVATIONS_DIR = os.path.join(CONCEPT_OUTPUT_DIR, 'activations')\n",
    "RESULTS_DIR = os.path.join(CONCEPT_OUTPUT_DIR, 'probe_results')\n",
    "PROBE_SAVE_DIR = os.path.join(CONCEPT_OUTPUT_DIR, 'saved_probes')\n",
    "os.makedirs(BASE_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(ACTIVATIONS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(PROBE_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_CONCEPTS = [\n",
    "    \"fork\", \"pin\", \"skewer\", \"discoveredAttack\", \"sacrifice\",\n",
    "    \"hangingPiece\", \"kingsideAttack\", \"exposedKing\", \"attraction\",\n",
    "    \"deflection\", \"interference\", \"clearance\", \"intermezzo\",\n",
    "    \"advancedPawn\", \"attackingF2F7\", \"capturingDefender\", \"doubleCheck\",\n",
    "    \"queensideAttack\", \"trappedPiece\", \"defensiveMove\", \"quietMove\",\n",
    "    \"mate\"\n",
    "]\n",
    "\n",
    "print(f\"Target Concepts: {TARGET_CONCEPTS}\")\n",
    "\n",
    "TARGET_LAYERS = [10]\n",
    "PUZZLE_LIMIT_PER_CLASS = 25000\n",
    "ACTIVATION_CHUNK_SIZE = 5000\n",
    "TEST_SPLIT_RATIO = 0.2\n",
    "PROBE_EPOCHS = 150\n",
    "PROBE_PATIENCE = 10\n",
    "PROBE_BATCH_SIZE = 512\n",
    "PROBE_LR = 0.0001\n",
    "PROBE_CONV_HIDDEN = 32\n",
    "PROBE_DROPOUT = 0.4\n",
    "PROBE_L2_DECAYS = [0.0, 1e-5] \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(f\"Target Layers: {TARGET_LAYERS}\")\n",
    "\n",
    "csv_save_path = os.path.join(RESULTS_DIR, f\"probe_evaluation_summary.csv\")\n",
    "json_save_path = os.path.join(RESULTS_DIR, f\"probe_evaluation_detailed.json\")\n",
    "\n",
    "completed_concepts = set()\n",
    "if os.path.exists(csv_save_path):\n",
    "    print(f\"--- Loading existing summary from: {csv_save_path} ---\")\n",
    "    try:\n",
    "        summary_df = pd.read_csv(csv_save_path)\n",
    "        completed_concepts = set(summary_df['concept'].unique())\n",
    "        print(f\"Found {len(completed_concepts)} completed concepts: {completed_concepts}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading summary CSV, starting fresh: {e}\")\n",
    "        summary_df = pd.DataFrame() # Start fresh if loading fails\n",
    "else:\n",
    "    print(\"--- No existing summary file found, starting fresh. ---\")\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "detailed_results = {}\n",
    "if os.path.exists(json_save_path):\n",
    "     print(f\"--- Loading existing detailed results from: {json_save_path} ---\")\n",
    "     try:\n",
    "         with open(json_save_path, 'r') as f:\n",
    "              detailed_results = json.load(f)\n",
    "         # Ensure keys are consistent with completed_concepts found in CSV\n",
    "         concepts_in_json = set(detailed_results.keys())\n",
    "         if concepts_in_json != completed_concepts:\n",
    "              print(\"Warning: Concepts in JSON and CSV differ. Rebuilding JSON from scratch.\")\n",
    "              detailed_results = {} # Start fresh if inconsistent\n",
    "     except Exception as e:\n",
    "          print(f\"Error loading detailed JSON, starting fresh: {e}\")\n",
    "          detailed_results = {}\n",
    "\n",
    "\n",
    "for concept_name in TARGET_CONCEPTS:\n",
    "\n",
    "    if concept_name in completed_concepts:\n",
    "        print(f\"\\n--- Skipping already completed concept: {concept_name} ---\")\n",
    "        if concept_name not in detailed_results:\n",
    "             print(f\"  Attempting to recover metrics for {concept_name} from probe files...\")\n",
    "             recovered_layer_data = {}\n",
    "             for layer_idx in TARGET_LAYERS:\n",
    "                 probe_path = os.path.join(PROBE_SAVE_DIR, f\"{concept_name}_layer_{layer_idx}_probe.pth\")\n",
    "                 if os.path.exists(probe_path):\n",
    "                     try:\n",
    "                         probe_data = torch.load(probe_path, map_location='cpu') # Load to CPU\n",
    "                         recovered_layer_data[str(layer_idx)] = { # Use string key for JSON consistency if needed\n",
    "                             \"validation_results\": {\n",
    "                                 'best_avg_val_accuracy': probe_data.get('best_avg_val_accuracy', np.nan),\n",
    "                                 'best_reg_param': probe_data.get('best_reg_param'),\n",
    "                                 'reg_param_name': probe_data.get('best_reg_param_name')\n",
    "                             },\n",
    "                             \"test_metrics\": probe_data.get('test_metrics')\n",
    "                         }\n",
    "                     except Exception as load_err:\n",
    "                         print(f\"    Error loading probe file {probe_path}: {load_err}\")\n",
    "                 else:\n",
    "                     print(f\"    Probe file not found: {probe_path}\")\n",
    "             if recovered_layer_data:\n",
    "                 detailed_results[concept_name] = recovered_layer_data\n",
    "                 print(f\"  Successfully recovered metrics for {concept_name}.\")\n",
    "        continue # Move to the next concept in the list\n",
    "\n",
    "    # --- Concept Not Completed - Proceed with Processing ---\n",
    "    print(f\"\\n{'='*15} Processing Concept: {concept_name} {'='*15}\")\n",
    "\n",
    "    current_concept_failed = False\n",
    "    save_path_prefix = f\"{concept_name}\"\n",
    "    data_save_base = os.path.join(BASE_DATA_DIR, save_path_prefix)\n",
    "    activation_dir_concept = os.path.join(ACTIVATIONS_DIR, save_path_prefix)\n",
    "    os.makedirs(activation_dir_concept, exist_ok=True)\n",
    "\n",
    "    pos_npz_path = f\"{data_save_base}_pos.npz\"\n",
    "    neg_npz_path = f\"{data_save_base}_neg.npz\"\n",
    "\n",
    "    # --- Step 1: Data Generation ---\n",
    "    print(f\"\\n=== STEP 1: Data Processing for '{concept_name}' ===\")\n",
    "    try:\n",
    "        # Check if data already exists (maybe from a previous partial run that crashed before cleanup)\n",
    "        if not (os.path.exists(pos_npz_path) and os.path.exists(neg_npz_path)):\n",
    "            data_generated = process_lichess_puzzles(\n",
    "                csv_path=LICHESS_DB_PATH,\n",
    "                target_tag=concept_name,\n",
    "                save_path_base=data_save_base,\n",
    "                limit=PUZZLE_LIMIT_PER_CLASS\n",
    "            )\n",
    "            if not data_generated:\n",
    "                print(f\"Data generation failed for {concept_name}. Skipping.\")\n",
    "                current_concept_failed = True\n",
    "        else:\n",
    "            print(f\"  Found existing NPZ data for {concept_name}. Skipping generation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data generation check/run for {concept_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        current_concept_failed = True\n",
    "\n",
    "    if current_concept_failed: continue\n",
    "\n",
    "    # --- Step 2: Activation Extraction ---\n",
    "    print(f\"\\n=== STEP 2: Activation Extraction for '{concept_name}' ===\")\n",
    "    success_pos, success_neg = False, False\n",
    "    # Check if activations already exist\n",
    "    all_activations_exist = True\n",
    "    pos_npy_files_paths = {}\n",
    "    neg_npy_files_paths = {}\n",
    "    for layer_idx in TARGET_LAYERS:\n",
    "         pos_npy_path = os.path.join(activation_dir_concept, f\"{save_path_prefix}_pos_layer_{layer_idx}_activations.npy\")\n",
    "         neg_npy_path = os.path.join(activation_dir_concept, f\"{save_path_prefix}_neg_layer_{layer_idx}_activations.npy\")\n",
    "         pos_npy_files_paths[layer_idx] = pos_npy_path\n",
    "         neg_npy_files_paths[layer_idx] = neg_npy_path\n",
    "         if not (os.path.exists(pos_npy_path) and os.path.exists(neg_npy_path)):\n",
    "             all_activations_exist = False\n",
    "             break\n",
    "\n",
    "    if all_activations_exist:\n",
    "        print(f\"  Found existing activation files for {concept_name}. Skipping extraction.\")\n",
    "        success_pos, success_neg = True, True\n",
    "    else:\n",
    "        print(f\"  Did not find all activation files. Running extraction...\")\n",
    "        try:\n",
    "            print(f\"--- Extracting Positive Activations ---\")\n",
    "            success_pos, _ = extract_and_save_activations_batched(\n",
    "                model_path=ONNX_MODEL_PATH, npz_data_path=pos_npz_path, target_layer_indices=TARGET_LAYERS, \n",
    "                output_dir=activation_dir_concept,output_prefix=f\"{save_path_prefix}_pos\",max_samples=None, \n",
    "                chunk_size=ACTIVATION_CHUNK_SIZE\n",
    "            )\n",
    "            if not success_pos:\n",
    "                print(f\"Positive activation extraction failed for {concept_name}. Skipping.\")\n",
    "                current_concept_failed = True\n",
    "\n",
    "            if not current_concept_failed:\n",
    "                print(f\"--- Extracting Negative Activations ---\")\n",
    "                success_neg, _ = extract_and_save_activations_batched(\n",
    "                    model_path=ONNX_MODEL_PATH, npz_data_path=neg_npz_path,\n",
    "                    target_layer_indices=TARGET_LAYERS, output_dir=activation_dir_concept,\n",
    "                    output_prefix=f\"{save_path_prefix}_neg\",max_samples=None, chunk_size=ACTIVATION_CHUNK_SIZE\n",
    "                )\n",
    "                if not success_neg:\n",
    "                    print(f\"Negative activation extraction failed for {concept_name}. Skipping.\")\n",
    "                    current_concept_failed = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error during activation extraction for {concept_name}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            current_concept_failed = True\n",
    "\n",
    "    if current_concept_failed: # Cleanup needed if extraction failed\n",
    "        print(f\"--- Attempting cleanup for {concept_name} after failure ---\")\n",
    "        pos_npy_files = list(pos_npy_files_paths.values())\n",
    "        neg_npy_files = list(neg_npy_files_paths.values())\n",
    "        files_to_delete = [pos_npz_path, neg_npz_path] + pos_npy_files + neg_npy_files\n",
    "        # ... (rest of cleanup logic from previous script) ...\n",
    "        for file_path in files_to_delete:\n",
    "             try:\n",
    "                 if os.path.exists(file_path): os.remove(file_path)\n",
    "             except OSError as err: print(f\"  Error deleting {file_path}: {err}\")\n",
    "        try:\n",
    "              if os.path.exists(activation_dir_concept) and not os.listdir(activation_dir_concept): os.rmdir(activation_dir_concept)\n",
    "        except OSError as err: print(f\"  Error removing activation directory {activation_dir_concept}: {err}\")\n",
    "        continue\n",
    "\n",
    "    # --- Step 3: Train & Evaluate Probes ---\n",
    "    print(f\"\\n=== STEP 3: Probe Training & Evaluation for '{concept_name}' ===\")\n",
    "    concept_results = None\n",
    "    try:\n",
    "        concept_results = perform_concept_detection_pytorch_probe(\n",
    "            target_layer_indices=TARGET_LAYERS, concept_name=save_path_prefix,\n",
    "            activations_dir=activation_dir_concept,\n",
    "            probe_save_dir=PROBE_SAVE_DIR, conv_hidden_channels=PROBE_CONV_HIDDEN,\n",
    "            dropout_prob=PROBE_DROPOUT, test_ratio=TEST_SPLIT_RATIO,\n",
    "            batch_size=PROBE_BATCH_SIZE, epochs=PROBE_EPOCHS,\n",
    "            learning_rate=PROBE_LR, patience=PROBE_PATIENCE,\n",
    "            l2_weight_decays=PROBE_L2_DECAYS,\n",
    "            random_seed=RANDOM_SEED\n",
    "        )\n",
    "\n",
    "        # --- Step 3.5: Update Results and Save Incrementally ---\n",
    "        if concept_results:\n",
    "            detailed_results[concept_name] = concept_results\n",
    "            # Prepare summary rows for this concept\n",
    "            concept_summary_rows = []\n",
    "            for layer_idx, results in concept_results.items():\n",
    "                val_res = results.get(\"validation_results\", {})\n",
    "                test_res = results.get(\"test_metrics\") \n",
    "                val_acc = val_res.get('best_avg_val_accuracy', np.nan) if val_res else np.nan\n",
    "\n",
    "                summary_row = {\n",
    "                    'concept': concept_name, 'layer': layer_idx,\n",
    "                    'best_val_acc': val_acc,\n",
    "                    'test_accuracy': test_res['accuracy'] if test_res else np.nan,\n",
    "                    'test_precision': test_res['precision'] if test_res else np.nan,\n",
    "                    'test_recall': test_res['recall'] if test_res else np.nan,\n",
    "                    'test_specificity': test_res['specificity'] if test_res else np.nan,\n",
    "                    'test_f1': test_res['f1_score'] if test_res else np.nan,\n",
    "                    'probe_file': os.path.join(PROBE_SAVE_DIR, f\"{concept_name}_layer_{layer_idx}_probe.pth\") if test_res else None\n",
    "                }\n",
    "                concept_summary_rows.append(summary_row)\n",
    "\n",
    "            # Append new rows to the main DataFrame and save\n",
    "            if concept_summary_rows:\n",
    "                new_rows_df = pd.DataFrame(concept_summary_rows)\n",
    "                if not summary_df.empty and 'concept' in summary_df.columns:\n",
    "                    summary_df = summary_df[summary_df['concept'] != concept_name]\n",
    "                    \n",
    "                summary_df = pd.concat([summary_df, new_rows_df], ignore_index=True)\n",
    "                try:\n",
    "                    summary_df.to_csv(csv_save_path, index=False)\n",
    "                    print(f\"  Successfully updated summary CSV: {csv_save_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error saving updated summary CSV: {e}\")\n",
    "\n",
    "            # Save the updated detailed results JSON\n",
    "            try:\n",
    "                with open(json_save_path, 'w') as f:\n",
    "                    json.dump(detailed_results, f, indent=4, default=lambda x: str(x) if isinstance(x, (np.generic, np.ndarray)) else None)\n",
    "                print(f\"  Successfully updated detailed JSON: {json_save_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error saving updated detailed JSON: {e}\")\n",
    "        else:\n",
    "             print(f\"  Probe training function returned no results for {concept_name}.\")\n",
    "             current_concept_failed = True # Treat as failure for cleanup\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during probe training/evaluation for {concept_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        current_concept_failed = True\n",
    "\n",
    "    # --- Step 4: Cleanup Intermediate Files ---\n",
    "    print(f\"\\n=== STEP 4: Cleaning up intermediate files for '{concept_name}' ===\")\n",
    "    pos_npy_files = [os.path.join(activation_dir_concept, f\"{save_path_prefix}_pos_layer_{l}_activations.npy\") for l in TARGET_LAYERS]\n",
    "    neg_npy_files = [os.path.join(activation_dir_concept, f\"{save_path_prefix}_neg_layer_{l}_activations.npy\") for l in TARGET_LAYERS]\n",
    "    files_to_delete = [pos_npz_path, neg_npz_path] + pos_npy_files + neg_npy_files\n",
    "    # ... (rest of cleanup logic from previous script) ...\n",
    "    for file_path in files_to_delete:\n",
    "         try:\n",
    "             if os.path.exists(file_path):\n",
    "                 os.remove(file_path)\n",
    "                 print(f\"  Deleted: {file_path}\")\n",
    "         except OSError as err: print(f\"  Error deleting {file_path}: {err}\")\n",
    "    try:\n",
    "          if os.path.exists(activation_dir_concept) and not os.listdir(activation_dir_concept):\n",
    "              os.rmdir(activation_dir_concept)\n",
    "              print(f\"  Removed empty activation directory: {activation_dir_concept}\")\n",
    "    except OSError as err: print(f\"  Error removing activation directory {activation_dir_concept}: {err}\")\n",
    "\n",
    "\n",
    "    print(f\"{'='*15} Finished Concept: {concept_name} {'='*15}\\n\")\n",
    "\n",
    "\n",
    "# === Final Summary (Optional - files are already saved incrementally) ===\n",
    "print(\"\\n\\n\" + \"=\"*20 + \" FINAL SUMMARY (from final state) \" + \"=\"*20)\n",
    "# Display final summary from the DataFrame\n",
    "print(summary_df.to_string())\n",
    "print(f\"\\nSummary saved to: {csv_save_path}\")\n",
    "print(f\"Detailed results saved to: {json_save_path}\")\n",
    "print(\"\\nSequential probe training and evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

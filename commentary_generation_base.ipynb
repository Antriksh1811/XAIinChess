{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from processing.static_concept_detection import get_structured_engine_evaluation\n",
    "from processing.concept_generation import analyze_puzzle_concept_importance\n",
    "from processing.models import Stockfish\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "stockfish_8_path = \"Stockfish-8/Windows/stockfish_8_x64.exe\"\n",
    "stockfish_engine = Stockfish(path=stockfish_8_path, parameters={\"Threads\": 1})\n",
    "\n",
    "# --- Initialization ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline + Engine\n",
    "PUZZLE_CSV_PATH = 'subset_puzzles_for_commentary.csv'\n",
    "OUTPUT_JSONL_PATH = 'commentary_gen/generated_commentaries_baseline_engine.jsonl'\n",
    "STOCKFISH_PATH = \"Stockfish-8/Windows/stockfish_8_x64.exe\"\n",
    "LLM_MODEL = \"gpt-4o\"\n",
    "\n",
    "# --- Initialization ---\",\n",
    "client = OpenAI()\n",
    "\n",
    "base_prompt_zeroshot = [\n",
    "{\n",
    "\"role\": \"system\",\n",
    "\"content\": \"\"\"\n",
    "You are an expert chess commentator generating commentary for a club-level player. Explain the short tactical sequence **strictly from the perspective of the player making the `target_move`**.\n",
    "Your goal is to provide a concise, clear, and fluid explanation focusing **purely on the chess actions, tactics, and consequences on the board.**\n",
    "Use the board states, the moves provided, and the stockfish evaluation to accomplish this.\n",
    "\n",
    "**Commentary Generation (Your Actual Output):**\n",
    "- Write a fluid commentary describing the sequence **only in terms of chess moves, piece interactions, threats, captures, and resulting advantages.**\n",
    "\n",
    "**Output Format:**\n",
    "- Output ONLY the `**Commentary:**` section.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "]\n",
    "\n",
    "print(f\"Loading puzzles from: {PUZZLE_CSV_PATH}\")\n",
    "df_puzzles = pd.read_csv(PUZZLE_CSV_PATH)\n",
    "\n",
    "print(f\"Found {len(df_puzzles)} puzzles.\")\n",
    "generated_count = 0\n",
    "\n",
    "\n",
    "with open(OUTPUT_JSONL_PATH, 'a') as outfile:\n",
    "    for index, row in tqdm(df_puzzles.iterrows(), total=len(df_puzzles), desc=\"Generating Zero-Shot Commentaries\"):\n",
    "        try: \n",
    "            puzzle_id = row['PuzzleId']\n",
    "            initial_fen = row['FEN']\n",
    "            moves_str = row['Moves']\n",
    "            puzzle_moves_uci = moves_str.split()\n",
    "\n",
    "            engine_eval = get_structured_engine_evaluation(\n",
    "                initial_fen,\n",
    "                puzzle_moves_uci,\n",
    "                stockfish_engine,\n",
    "                depth=20\n",
    "            )\n",
    "            if not engine_eval: \n",
    "                 print(f\"Skipping Puzzle {puzzle_id} due to engine evaluation error.\")\n",
    "                 continue\n",
    "\n",
    "            llm_payload_zeroshot = {\n",
    "                \"target_fen_s0\": engine_eval.get('target_fen'),\n",
    "                \"target_move\": engine_eval.get(\"target_move_dict\"),\n",
    "                \"opponent_reply\": engine_eval.get(\"opponent_reply_dict\"),\n",
    "                \"target_fen_s2\": engine_eval.get('fen_state_2'),\n",
    "                \"best_move_after_s2\": engine_eval.get(\"best_move_after_s2_dict\")\n",
    "            }\n",
    "            if not all(llm_payload_zeroshot.values()):\n",
    "                 print(f\"Skipping Puzzle {puzzle_id} due to missing data in engine_eval.\")\n",
    "                 continue\n",
    "            \n",
    "            current_prompt = base_prompt_zeroshot + [{\"role\": \"user\", \"content\": json.dumps(llm_payload_zeroshot, indent=2)}]\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_MODEL,\n",
    "                messages=current_prompt,\n",
    "                temperature=0.01 \n",
    "            )\n",
    "            llm_response_content = response.choices[0].message.content.strip() \n",
    "            print(f\"Puzzle {puzzle_id}: {llm_response_content}\\n\") \n",
    "\n",
    "            result_record = {\n",
    "                \"puzzle_id\": puzzle_id,\n",
    "                \"initial_fen\": initial_fen,\n",
    "                \"moves_uci\": puzzle_moves_uci,\n",
    "                \"target_move_san\": engine_eval.get('target_move_dict'),\n",
    "                \"fen_s0\": engine_eval.get('target_fen'),\n",
    "                \"fen_s2\": engine_eval.get('fen_state_2'),\n",
    "                \"llm_commentary\": llm_response_content \n",
    "            }\n",
    "            outfile.write(json.dumps(result_record) + '\\n')\n",
    "            generated_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"\\n--- Error processing Puzzle ID {puzzle_id} (Index {index}) ---\")\n",
    "             print(f\"FEN: {initial_fen}\")\n",
    "             print(f\"Moves: {moves_str}\")\n",
    "             print(f\"Error: {e}\")\n",
    "             # Optionally add a delay or specific error handling here\n",
    "             # time.sleep(1) # Example delay\n",
    "\n",
    "\n",
    "print(f\"\\nFinished generation. Generated {generated_count} zero-shot commentaries.\")\n",
    "# --- CHANGE: Update final message ---\n",
    "print(f\"Results saved to: {OUTPUT_JSONL_PATH}\")\n",
    "\n",
    "# --- END OF FILE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just baseline\n",
    "PUZZLE_CSV_PATH = 'subset_puzzles_for_commentary.csv' \n",
    "OUTPUT_JSONL_PATH = 'commentary_gen/generated_commentaries_baseline_test.jsonl' \n",
    "ONNX_MODEL_PATH = 'prepared_network_fp16.onnx' \n",
    "PROBE_SAVE_DIR = 'concepts_sequential/saved_probes' \n",
    "STOCKFISH_PATH = \"Stockfish-8/Windows/stockfish_8_x64.exe\" \n",
    "\n",
    "# LLM & Probe Config\n",
    "LLM_MODEL = \"gpt-4o\" \n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "base_prompt = [\n",
    "{\n",
    "\"role\": \"system\",\n",
    "\"content\": \"\"\"\n",
    "You are an expert chess commentator generating commentary for a club-level player. Explain the short tactical sequence **strictly from the perspective of the player making the `target_move`**.\n",
    "Your goal is to provide a concise, clear, and fluid explanation focusing **purely on the chess actions, tactics, and consequences on the board.**\n",
    "Use the board states, and the moves given to accomplish this.\n",
    "\n",
    "**Commentary Generation (Your Actual Output):**\n",
    "- Write a fluid commentary describing the sequence **only in terms of chess moves, piece interactions, threats, captures, and resulting advantages.**\n",
    "\n",
    "\"\"\"\n",
    "},\n",
    "]\n",
    "\n",
    "# --- Main Processing Logic ---\n",
    "print(f\"Loading puzzles from: {PUZZLE_CSV_PATH}\")\n",
    "df_puzzles = pd.read_csv(PUZZLE_CSV_PATH)\n",
    "\n",
    "print(f\"Found {len(df_puzzles)} puzzles.\")\n",
    "generated_count = 0\n",
    "\n",
    "# Open output file in append mode\n",
    "with open(OUTPUT_JSONL_PATH, 'a') as outfile:\n",
    "    for index, row in tqdm(df_puzzles.iterrows(), total=len(df_puzzles), desc=\"Generating Commentaries\"):\n",
    "        puzzle_id = row['PuzzleId'] \n",
    "        initial_fen = row['FEN']\n",
    "        moves_str = row['Moves']\n",
    "        puzzle_moves_uci = moves_str.split()\n",
    "\n",
    "        # 2. Get structured engine evaluation\n",
    "        engine_eval = get_structured_engine_evaluation(\n",
    "            initial_fen,\n",
    "            puzzle_moves_uci,\n",
    "            stockfish_engine,\n",
    "            depth=20 \n",
    "        )\n",
    "\n",
    "        # 3. Prepare LLM Payload\n",
    "        llm_payload = {\n",
    "            \"target_fen_s0\": engine_eval['target_fen'],\n",
    "            \"target_move\": engine_eval[\"target_move_dict\"],\n",
    "            \"opponent_reply\": engine_eval[\"opponent_reply_dict\"],\n",
    "            \"target_fen_s2\": engine_eval['fen_state_2'],\n",
    "            \"best_move_after_s2\": engine_eval[\"best_move_after_s2_dict\"],\n",
    "        }\n",
    "\n",
    "        # 4. Construct final prompt\n",
    "        current_prompt = base_prompt + [{\"role\": \"user\", \"content\": json.dumps(llm_payload, indent=2)}]\n",
    "\n",
    "        # 5. Call LLM API\n",
    "        response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=current_prompt,\n",
    "            temperature=0.01 \n",
    "        )\n",
    "        llm_response_content = response.choices[0].message.content\n",
    "        print(llm_response_content)\n",
    "\n",
    "        # 6. Parse LLM Response (basic split)\n",
    "        commentary = llm_response_content \n",
    "\n",
    "        # 7. Store Result\n",
    "        result_record = {\n",
    "            \"puzzle_id\": puzzle_id,\n",
    "            \"initial_fen\": initial_fen,\n",
    "            \"moves_uci\": puzzle_moves_uci,\n",
    "            \"target_move_san\": engine_eval['target_move_dict'],\n",
    "            \"fen_s0\": engine_eval['target_fen'],\n",
    "            \"best_move_after_s2\": engine_eval[\"best_move_after_s2_dict\"],\n",
    "            \"fen_s2\": engine_eval['fen_state_2'],\n",
    "            \"llm_commentary\": commentary\n",
    "        }\n",
    "        outfile.write(json.dumps(result_record) + '\\n')\n",
    "        generated_count += 1\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nFinished generation. Generated {generated_count} commentaries.\")\n",
    "print(f\"Results saved to: {OUTPUT_JSONL_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
